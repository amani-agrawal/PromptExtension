{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTO4//q0Faw7W6PFfV1C9T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amani-agrawal/PromptExtension/blob/main/PromptExtension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies:\n",
        "\n",
        "pip install -q bitsandbytes accelerate datasets peft transformers sentencepiece"
      ],
      "metadata": {
        "id": "eyIAAx9rILtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load your base model"
      ],
      "metadata": {
        "id": "xi9KS5dlIrOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "\n",
        "MODEL_NAME = \"google/flan-t5-large\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, device_map=\"auto\", load_in_8bit=True)\n",
        "\n",
        "input_text = \"translate English to German: How old are you?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "USUbtphFIQWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load your dataset"
      ],
      "metadata": {
        "id": "MXlvD62bIq2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# paths to the files\n",
        "data_files = {\n",
        "    \"train\": \"train.csv\",\n",
        "    \"validation\": \"val.csv\",\n",
        "}\n",
        "\n",
        "raw = load_dataset(\"csv\", data_files=data_files)\n",
        "\n",
        "# Column headers are \"input_text\" and \"output_text\"."
      ],
      "metadata": {
        "id": "fXI4g7CWIxju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Convert the sentences to a tokenised format for encoder(input ids and attention_mask) and decoder(input_ids/labels)"
      ],
      "metadata": {
        "id": "t7Tdg1-XJHW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(batch):\n",
        "    model_in = tokenizer(\n",
        "        batch[\"input_text\"],\n",
        "        truncation=True,\n",
        "        padding=\"longest\",\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            batch[\"output_text\"],\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "        )\n",
        "    model_in[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_in\n",
        "\n",
        "tokenised = raw.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=raw[\"train\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "QeylmrYIJNyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create a set of layers which will be fine tuned on the base model"
      ],
      "metadata": {
        "id": "-Rd77vert4wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],  # good trade‑off for T5\n",
        "    lora_dropout=0.05,\n",
        ")\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "model.print_trainable_parameters()      # sanity check (~12 M params)\n"
      ],
      "metadata": {
        "id": "0rcdbXgvTZPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Set the parameters for training the new model and saving it"
      ],
      "metadata": {
        "id": "I0aX3_l6z_Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir          = \"./lora-flan-t5-large\",\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size  = 8,\n",
        "    gradient_accumulation_steps = 4,    # effective batch = 32\n",
        "    num_train_epochs    = 3,\n",
        "    learning_rate       = 2e-4,\n",
        "    lr_scheduler_type   = \"cosine\",\n",
        "    fp16                = True,\n",
        "    logging_steps       = 50,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy       = \"epoch\",\n",
        "    report_to           = \"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model          = model,\n",
        "    args           = training_args,\n",
        "    train_dataset  = tokenised[\"train\"],\n",
        "    eval_dataset   = tokenised[\"validation\"],\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"./lora-flan-t5-large\")\n",
        "tokenizer.save_pretrained(\"./lora-flan-t5-large\")"
      ],
      "metadata": {
        "id": "FtNeKF0cTn_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Finally load the model and run the prompt"
      ],
      "metadata": {
        "id": "_a2VWA-55H19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "base  = T5ForConditionalGeneration.from_pretrained(\n",
        "           MODEL_NAME, load_in_8bit=True, device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(base, \"./lora-flan-t5-large\").eval()\n",
        "\n",
        "prompt = \"translate English to German: How old are you?\"\n",
        "ids    = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "print(tokenizer.decode(model.generate(ids)[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "5LnONEwQTuYG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}